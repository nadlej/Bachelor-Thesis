{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_second_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WpM_nBIMcw1"
      },
      "source": [
        "!pip3 install pickle5\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sn\n",
        "import pickle5 as pickle\n",
        "from datetime import datetime \n",
        "from pylab import savefig\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive \n",
        "from numpy import load, save\n",
        "from keras import regularizers, activations\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwwbenbSUcNt"
      },
      "source": [
        "Importowanie bibliotek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKJzY54SNCmz",
        "outputId": "e59ec378-17ef-438e-fdcd-2616e016afee"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "us8k_df = pd.read_pickle(\"/content/gdrive/MyDrive/Licencjat/us8k_df.pkl\")\n",
        "\n",
        "sound_classes_short = ['WEN', 'KLAK', 'DZIE', 'PIES', 'WIER',\n",
        "                      'SIL', 'STRZ', 'MŁOT', 'SYR', 'UL']  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdZlMViHUd_0"
      },
      "source": [
        "Połaczenie z Google Drive'm oraz wczytanie pliku w formacie \"pickle\" zawierającym zapis spektrogramów przeskalowanych w skali melowej wraz z odpowiadającymi oznaczeniami folderów i klas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a7GLi9uNGF7"
      },
      "source": [
        "def init_model():\n",
        "    model1 = Sequential()\n",
        "    \n",
        "    #layer-1\n",
        "    model1.add(Conv2D(filters=24, kernel_size=5, input_shape=(128, 128, 1),\n",
        "                      kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    model1.add(MaxPooling2D(pool_size=(3,3), strides=3))\n",
        "    model1.add(Activation(activations.relu))\n",
        "    \n",
        "    #layer-2\n",
        "    model1.add(Conv2D(filters=36, kernel_size=4, padding='valid',\n",
        "                      kernel_regularizer=regularizers.l2(1e-3)))\n",
        "    model1.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model1.add(Activation(activations.relu))\n",
        "    \n",
        "    #layer-3\n",
        "    model1.add(Conv2D(filters=48, kernel_size=3, padding='valid'))\n",
        "    model1.add(Activation(activations.relu))\n",
        "    \n",
        "    model1.add(GlobalAveragePooling2D())\n",
        "    \n",
        "    #layer-4\n",
        "    model1.add(Dense(60, activation='relu'))\n",
        "    model1.add(Dropout(0.5))\n",
        "    \n",
        "    #layer-5\n",
        "    model1.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # compile\n",
        "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "    \n",
        "    return model1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJFD11_2Uw2E"
      },
      "source": [
        "Inicjalizacja modelu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvLJZPmiNGsT"
      },
      "source": [
        "model = init_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU2Cuj3YUy0s"
      },
      "source": [
        "Funkcja dzielenia folderów na zbior walidacyjny, testowy oraz treningowy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbtzhvUqNNQb"
      },
      "source": [
        "def train_test_split(fold_k, data, X_dim=(128, 128, 1)):\n",
        "  train_set = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
        "  train_set.remove(fold_k)\n",
        "  val_fold = random.choice(train_set)\n",
        "  \n",
        "  X_train = data[data.folder_id != val_fold]\n",
        "  X_train = np.stack(X_train[X_train.folder_id != fold_k].melspectrogram.to_numpy())\n",
        "  X_val = np.stack(data[data.folder_id == val_fold].melspectrogram.to_numpy())\n",
        "  X_test = np.stack(data[data.folder_id == fold_k].melspectrogram.to_numpy())\n",
        "\n",
        "  y_train = data[data.folder_id != fold_k]\n",
        "  y_train = y_train[y_train.folder_id != val_fold].audio_class_id.to_numpy()\n",
        "  y_val = data[data.folder_id == val_fold].audio_class_id.to_numpy()\n",
        "  y_test = data[data.folder_id == fold_k].audio_class_id.to_numpy()\n",
        "\n",
        "  XX_train = X_train.reshape(X_train.shape[0], *X_dim)\n",
        "  XX_val = X_val.reshape(X_val.shape[0], *X_dim)\n",
        "  XX_test = X_test.reshape(X_test.shape[0], *X_dim)\n",
        "  \n",
        "  yy_train = to_categorical(y_train)\n",
        "  yy_val = to_categorical(y_val)\n",
        "  yy_test = to_categorical(y_test)\n",
        "  \n",
        "  return XX_train, XX_val, XX_test, yy_train, yy_val, yy_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Ne5q16VcxE"
      },
      "source": [
        "Funkcja rysująca wykres zmiany dokładności i straty w funkcji liczby epok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-eOcTogpRO9"
      },
      "source": [
        "def show_results(histories):\n",
        "    for i, history in enumerate(histories):\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        plt.subplot(121)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.grid(linestyle='--')\n",
        "        plt.ylabel('Dokładność')\n",
        "        plt.xlabel('Epoka')\n",
        "        plt.legend(['train', 'validation'], loc='upper left')\n",
        "        plt.title(\"Dokładność modelu na epokę\")\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.grid(linestyle='--')\n",
        "        plt.ylabel('Strata')\n",
        "        plt.xlabel('Epoka')\n",
        "        plt.legend(['train', 'validation'], loc='upper left')\n",
        "        plt.title(\"Strata modelu na epokę\")\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "        print('\\tMaksymalna dokładność zbioru walidacyjnego: %.4f %%' % (np.max(history.history['val_accuracy']) * 100))\n",
        "        print('\\tMinimalna strata zbioru walidacyjnego: %.5f' % np.min(history.history['val_loss']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Nt1qUPVYAl"
      },
      "source": [
        "Funkcja zwracająca dokładność modelu dla danych testowych oraz macierz pomyłek dla tych danych."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6h-V6LfNRHX"
      },
      "source": [
        "def evaluate(model, XX_test, yy_test):\n",
        "    y_prob = model.predict(XX_test, verbose=0)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    y_true = yy_test\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "    score, accuracy = model.evaluate(XX_test, yy_test, batch_size=32, verbose=0)\n",
        "\n",
        "    print(\"\\nDokładność = {:.2f}\".format(accuracy))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    return accuracy, cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAmtiSUdVlAd"
      },
      "source": [
        "Każdy folder po kolei jest wpisywany jako folder testowy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3-C8vupNRx0"
      },
      "source": [
        "TEST_FOLDER = '1'\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_test_split(TEST_FOLDER, us8k_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhk7rc7NVFx"
      },
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_flwL5INWW6"
      },
      "source": [
        "history_store = []\n",
        "\n",
        "start = datetime.now()\n",
        "    \n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=100,\n",
        "                    batch_size=32, \n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[EarlyStopping(restore_best_weights=True, patience=15)])\n",
        "end = datetime.now()\n",
        "history_store.append(history)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2f65q3XWBw0"
      },
      "source": [
        "Wyświetlanie rysunków."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRFgxnP4Vz2V"
      },
      "source": [
        "show_results(history_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Q5GrlFWDV8"
      },
      "source": [
        "Badanie dokładności dla danych testowych oraz zwrócenie macierzy pomyłek dla danego folderu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQAtqpUZOGwH"
      },
      "source": [
        "accuracy, cm = evaluate(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaAW08_gWFJk"
      },
      "source": [
        "Inicjalizowana jest zmienna \"CM\" oznaczająca macierz pomyłek. Zważając na to, że proces trenowania trwa chwilę, a trzeba go powtórzyć co najmniej 10 razy, macierz po zaktualizowaniu można zapisać, a następnie wczytać. Wówczas wykonywana jest jedynie część z następnych operacji - po zwróceniu macierzy pomyłek dla danego folderu, zapisanego w \"cm\" nie inicjalizujemy znowu \"CM\", a jedynie aktualizujemy jego wartości, dodając te z \"cm\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mBdk_obOH7W"
      },
      "source": [
        "CM = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5v7zsvJOIzI"
      },
      "source": [
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XPi0OCOKW4"
      },
      "source": [
        "CM = CM + cm\n",
        "save('confusion_matrix.npy', CM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGAUjD_5z-4D"
      },
      "source": [
        "CM = load('confusion_matrix.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBiCXQlRG3q"
      },
      "source": [
        "CM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUm83ZS_YAJV"
      },
      "source": [
        "Wyświetlenie macierzy pomyłek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHJK5A8XnXlp"
      },
      "source": [
        "df_cm = pd.DataFrame(CM, sound_classes_short, sound_classes_short)\n",
        "sn.set(font_scale=0.8)\n",
        "svm = sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10}, fmt=\".0f\")\n",
        "plt.figure(figsize = (20,14))\n",
        "figure = svm.get_figure()    \n",
        "figure.savefig('svm_conf.png', dpi=400)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}